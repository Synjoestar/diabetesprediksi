# -*- coding: utf-8 -*-
"""diabetesprediksi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kpo5JD0IuAU-W5xz-ig-o6oAAUgoxvUu

# DIABETES PREDIKSI

## Business Understanding

Membuat sebuah model machine learning untuk predictive analytics

## Data Understanding

Dalam proyek ini, dataset yang digunakan adalah [Dataset](https://www.kaggle.com/datasets/whenamancodes/predict-diabities). Dataset ini terdiri dari 768 baris dengan 9 kolom.

Berikut adalah rincian dari dataset tersebut:

| kolom                        | Deskripsi                                                           |
| ---------------------------- | ------------------------------------------------------------------- |
| **Pregnancies**              | Jumlah kehamilan yang pernah dialami                                |
| **Glucose**                  | Tingkat glukosa dalam darah                                         |
| **BloodPressure**            | Tekanan darah (dalam mm Hg)                                         |
| **SkinThickness**            | Ketebalan lipatan kulit (dalam mm)                                  |
| **Insulin**                  | Kadar insulin dalam darah (dalam mu U/ml)                           |
| **BMI**                      | Indeks massa tubuh (Body Mass Index)                                |
| **DiabetesPedigreeFunction** | Skor riwayat keturunan diabetes                                     |
| **Age**                      | Usia pasien (dalam tahun)                                           |
| **Outcome**                  | Hasil diagnosa (1 = Mengidap diabetes, 0 = Tidak mengidap diabetes) |

## import library
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

"""## Gathering Data"""

# Masukkan Dataset yang digunakan dan lihat 5 baris pertama nya
df = pd.read_csv('/content/diabetesprediksi.csv')
df.head()

"""## Data Assesing & Data Cleaning

### Data Assessing (Penilaian Data)
Pada tahap ini dilakukan proses eksplorasi terhadap struktur dan kualitas data. Fokus utamanya yaitu:

* Memeriksa apakah ada data yang hilang (missing values).

* Mengidentifikasi duplikasi data.

* Menemukan nilai-nilai yang tidak valid atau outlier.

* Menilai apakah data ini cocok untuk dianalisis lebih lanjut atau dibutuhkan penyesuaian terlebih dahulu.


### Data Cleaning (Pembersihan Data)
Tahap ini bertujuan untuk memperbaiki permasalahan yang ditemukan saat penilaian data. Langkah-langkah yang dilakukan antara lain:

* Mengisi atau menghapus nilai yang hilang sesuai konteks.

* Menghapus data duplikat agar tidak mempengaruhi analisis.

* Menyesuaikan format data agar seragam (misalnya desimal, satuan, kapitalisasi).

* Menangani nilai ekstrim atau tidak wajar agar tidak menyesatkan model.
"""

# Melihat jumlah baris dan kolom pada dataset
df.shape

# Menampilkan informasi tentang DataFrame, termasuk jumlah nilai non-null, tipe data, dan penggunaan memori
df.info()

# Menghitung jumlah nilai yang hilang (NaN) di setiap kolom DataFrame
df.isna().sum()

# Menghitung jumlah baris duplikat dalam DataFrame
df.duplicated().sum()

# Menampilkan statistik ringkasan dari DataFrame
df.describe()

"""Berdasarkan proses penilaian data (data assessing) dan pembersihan data (data cleaning) yang telah dilakukan, pada dataset yang digunakan tidak ditemukan nilai yang hilang (missing values) maupun data yang duplikat.

Kemudian, dengan menggunakan fungsi info(), kita dapat mengidentifikasi fitur-fitur yang terdapat dalam dataset, yang meliputi: Pregnancies, Glucose, Blood Pressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, dan Outcome.

Selain itu, dengan menggunakan fungsi describe(), kita bisa mendapatkan ringkasan statistik dari dataset yang digunakan, seperti nilai rata-rata (mean), standar deviasi (standard deviation), nilai minimum (min), nilai maksimum (max), dan lainnya.

## Exploratory Data Analysis (EDA)

Analisis Data Eksplorasi (Exploratory Data Analysis/EDA) adalah proses investigasi awal terhadap data untuk menganalisis karakteristik, menemukan pola, anomali, dan memeriksa asumsi dalam data. Teknik ini biasanya menggunakan bantuan statistik serta representasi grafis atau visualisasi.

Pada kode di bawah ini, kita akan mengetahui bagaimana perbandingan jumlah orang yang memiliki diabetes dan yang tidak memiliki diabetes.
"""

# Pisahkan data berdasarkan nilai Outcome
diabetes = df[df['Outcome'] == 1]
non_diabetes = df[df['Outcome'] == 0]

# Hitung jumlah data untuk masing-masing kelompok
num_diabetes = len(diabetes)
num_non_diabetes = len(non_diabetes)

# Buat label dengan tambahan informasi jumlah orang
labels = [f'Diabetes ({num_diabetes} orang)', f'Non-Diabetes ({num_non_diabetes} orang)']
sizes = [num_diabetes, num_non_diabetes]
colors = ['#F16767', '#FF9B17']
explode = (0.1, 0)  # Pisahkan potongan "Diabetes" sedikit

plt.figure(figsize=(8, 6))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140, wedgeprops={'edgecolor': 'black'})
plt.axis('equal')  # Pastikan pie chart berbentuk lingkaran
plt.title('Perbandingan Diabetes dan Non-Diabetes')
plt.show()

"""#### Membuat histogram untuk setiap kolom numerik dalam DataFrame"""

# Menentukan kolom numerik yang ada di DataFrame
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns

# Membuat histogram untuk setiap kolom numerik
plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_columns, 1):
    plt.subplot(3, 3, i)  # Mengatur layout subplot dalam grid 3x3
    df[col].hist(bins=20, color='#FF9B17', edgecolor='black')  # Menggunakan warna yang konsisten
    plt.title(f'Histogram: {col}')
    plt.xlabel(col)
    plt.ylabel('Frekuensi')

plt.tight_layout()  # Menyesuaikan layout agar tidak tumpang tindih
plt.show()

"""Kode di bawah ini bertujuan untuk menghitung korelasi antara kolom numerik dalam DataFrame df dan kemudian mengurutkannya berdasarkan korelasi dengan kolom 'Outcome'.

Jika koefisien korelasi mendekati +1, ini menunjukkan hubungan positif antara dua variabel. Artinya, ketika satu variabel meningkat, kemungkinan variabel lainnya juga akan meningkat. Sebaliknya, jika koefisien korelasi mendekati -1, ini menunjukkan hubungan negatif antara dua variabel. Artinya, ketika satu variabel meningkat, variabel lainnya cenderung menurun.

Jika koefisien korelasi mendekati 0, ini menunjukkan bahwa tidak ada hubungan linier antara dua variabel. Namun, penting untuk diingat bahwa tidak adanya korelasi linier tidak berarti tidak ada hubungan sama sekali; itu hanya berarti bahwa hubungan tersebut tidak dapat dijelaskan dengan cara yang sama seperti hubungan positif atau negatif.
"""

# pairplot
sns.pairplot(df, hue="Outcome")

# Membuat figure dengan ukuran yang lebih besar
plt.figure(figsize=(12, 10))

# Menghitung korelasi antar kolom numerik dalam dataframe
correlation_matrix = df.corr().round(2)

# Membuat heatmap untuk matriks korelasi dengan parameter untuk annotasi nilai di setiap kotak
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.8, fmt='.2f',
            cbar_kws={'shrink': 0.8}, square=True, linecolor='white')

# Menambahkan judul dan mengatur ukuran font
plt.title("Matriks Korelasi Antar Fitur Numerik", fontsize=20)

# Menampilkan plot
plt.show()

"""#### Dalam proyek ini, kita akan mendeteksi outlier menggunakan teknik visualisasi data boxplot. Kemudian, outlier ini akan ditangani menggunakan metode IQR (Inter Quartile Range)."""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4  # Menentukan jumlah baris berdasarkan jumlah kolom
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot before median imputation", fontsize=20)

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#FF9B17')
        axes[row, col].set_title(f"Box Plot - {column}", fontsize=14)

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

# Panggil fungsi untuk menampilkan box plot
box_plots_all_columns(df)

"""Dapat dilihat dari visualisasi data di atas bahwa terdapat outlier dalam dataset yang digunakan. Oleh karena itu, perlu dilakukan penanganan terhadap data outlier.

IQR (Interquartile Range) memberi kita gambaran tentang variasi dalam dataset. Setiap nilai yang berada di luar rentang $-1.5 \times IQR$ hingga $1.5 \times IQR$ dianggap sebagai outlier.

* **Q1** mewakili kuartil pertama (25th percentile) dari data.
* **Q2** mewakili kuartil kedua (median/50th percentile) dari data.
* **Q3** mewakili kuartil ketiga (75th percentile) dari data.
* $(Q1 - 1.5 \times IQR)$ mewakili nilai terkecil dalam dataset, dan $(Q3 + 1.5 \times IQR)$ mewakili nilai terbesar dalam dataset.

"""

# Menghitung Q1 (Kuartil pertama) dan Q3 (Kuartil ketiga)
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)

# Menghitung IQR (Interquartile Range)
IQR = Q3 - Q1

# Menyaring data dengan menghapus outlier berdasarkan IQR
df_cleaned = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Menampilkan dimensi dataframe setelah menghapus outlier
print(f"Dimensi dataframe setelah pembersihan outlier: {df_cleaned.shape}")

"""#### Menampilkan hasil visualisasi data setelah menerapkan teknik IQR"""

def box_plots_all_columns(df):
    num_cols = len(df.columns)
    num_rows = (num_cols + 3) // 4  # Menentukan jumlah baris berdasarkan jumlah kolom
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, num_rows * 4))
    plt.suptitle("Box Plot before median imputation", fontsize=20)

    for i, column in enumerate(df.columns):
        row = i // 4
        col = i % 4
        sns.boxplot(df[column], ax=axes[row, col], color='#FF9B17')
        axes[row, col].set_title(f"Box Plot - {column}", fontsize=14)

    for i in range(num_cols, num_rows * 4):
        fig.delaxes(axes.flatten()[i])

    plt.tight_layout()
    plt.show()

# Panggil fungsi untuk menampilkan box plot
box_plots_all_columns(df)

"""Berdasarkan hasil perbandingan sebelumnya antara jumlah orang dengan diabetes dan non-diabetes, terlihat bahwa jumlah data orang dengan diabetes hanya 34,9% dan non-diabetes 65,1%. Ini menunjukkan bahwa data yang digunakan tidak seimbang (imbalance data). Oleh karena itu, perlu dilakukan penanganan terhadap data yang tidak seimbang, karena data yang tidak seimbang dapat menghasilkan bias dalam model dan hasil akurasi yang tidak akurat.

Terdapat dua metode untuk menangani data yang tidak seimbang, yaitu oversampling dan undersampling, tergantung pada kasus dan dataset yang digunakan.

**Oversampling**: Digunakan ketika kita memiliki dataset kecil dan ingin mengambil sampel lebih banyak dari kelas minoritas. Oversampling dengan SMOTE dapat membantu meningkatkan akurasi model karena tidak menghilangkan data, tetapi bisa meningkatkan risiko overfitting jika tidak dikelola dengan baik.

**Undersampling**: Digunakan ketika kita memiliki dataset besar dan ingin mengurangi jumlah sampel dari kelas mayoritas. Undersampling dapat membantu mengurangi waktu pelatihan dan meningkatkan keseimbangan kelas, tetapi dapat mengurangi informasi yang berguna jika sampel kelas mayoritas dihapus secara acak.

Dalam kasus ini, kami akan menerapkan metode **oversampling** karena dataset yang digunakan termasuk dalam kategori kecil, sehingga dengan menggunakan metode ini dataset bisa menjadi seimbang.

"""

# Pisahkan fitur (X) dan target (y)
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Fungsi untuk melakukan oversampling menggunakan SMOTE
def apply_smote(X, y):
    smote = SMOTE(random_state=42)  # Inisialisasi SMOTE dengan random_state untuk reproduksibilitas
    X_resampled, y_resampled = smote.fit_resample(X, y)  # Melakukan resampling
    return X_resampled, y_resampled

# Terapkan SMOTE untuk oversampling
X_resampled, y_resampled = apply_smote(X, y)

# Hitung distribusi kelas setelah oversampling
class_distribution = Counter(y_resampled)

# Tampilkan distribusi kelas setelah oversampling
print("Distribusi kelas setelah oversampling:")
for label, count in class_distribution.items():
    print(f'Kelas {label}: {count} sampel')

# Hitung distribusi kelas sebelum SMOTE
counter_before = Counter(y)

# Oversampling menggunakan SMOTE
oversample = SMOTE(random_state=42)
X_over, y_over = oversample.fit_resample(X, y)
counter_over = Counter(y_over)

# Visualisasi untuk membandingkan distribusi sebelum dan setelah SMOTE
plt.figure(figsize=(12, 6))

# Sebelum SMOTE
plt.subplot(1, 2, 1)
colors = ['#FF9B17', '#F16767']  # Gunakan warna oranye dan hijau
plt.bar(counter_before.keys(), counter_before.values(), color=colors)
plt.xticks([0, 1])
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.title('Sebelum SMOTE')
plt.ylim([0, max(counter_before.values()) + 100])

for i, v in enumerate(counter_before.values()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

# Setelah SMOTE
plt.subplot(1, 2, 2)
plt.bar(counter_over.keys(), counter_over.values(), color=colors)
plt.xticks([0, 1])
plt.xlabel('Kelas')
plt.ylabel('Jumlah')
plt.title('Setelah SMOTE')
plt.ylim([0, max(counter_over.values()) + 100])

for i, v in enumerate(counter_over.values()):
    plt.text(i, v + 5, str(v), ha='center', va='bottom')

# Menata layout dan menampilkan plot
plt.tight_layout()
plt.show()

"""Data yang tidak seimbang (imbalanced data) telah berhasil ditangani, data sekarang sudah seimbang dan siap digunakan untuk tahap pengembangan model.

## Data splitting

Tahap selanjutnya adalah membagi dataset menjadi data latih (train) dan data uji (test). Data latih akan digunakan dalam proses pelatihan model, dan data uji akan digunakan untuk menguji atau mengetahui seberapa baik model yang telah dibuat dapat menggeneralisasi data baru yang belum pernah dilihat sebelumnya.

Setelah melakukan beberapa percobaan(sebelumya aku 80:20), dalam kasus ini dataset akan dibagi dengan proporsi 70:30, yaitu data latih (70%) dan data uji (30%).
"""

# Pisahkan dataset menjadi fitur (X) dan target (y)
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Pembagian data dengan proporsi 70:30
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Menampilkan bentuk data latih dan data uji
print("Data Latih (Train):", X_train.shape)
print("Data Uji (Test):", X_test.shape)

"""## Standardisasi

Sebelum mengembangkan model machine learning, proses standardisasi dilakukan terlebih dahulu. Tahapan ini digunakan untuk memproses fitur numerik dalam data agar memiliki nilai rata-rata 0 dan deviasi standar 1. Salah satu tujuan standardisasi adalah karena banyak algoritma machine learning yang bekerja lebih baik atau lebih stabil ketika fitur numerik berada dalam skala yang sama. Dengan standardisasi, fitur-fitur ini diperlakukan secara seragam, yang dapat meningkatkan kinerja model.
"""

# Membuat pipeline untuk standardisasi
num_pipeline = Pipeline([('std_scaler', StandardScaler())])

# Menerapkan standardisasi pada data latih
X_train_prepared = num_pipeline.fit_transform(X_train)

# Menerapkan standardisasi pada data uji (hanya transformasi, tanpa fitting)
X_test_prepared = num_pipeline.transform(X_test)

"""## Modelling & Evaluate"""

# Fungsi untuk mengevaluasi performa model dengan classification report dan confusion matrix
def evaluate(clf, X_train, y_train, X_test, y_test, train=False):
    # Ambil label unik dari data latih atau uji (untuk label confusion matrix nanti)
    classes = y_train.unique() if train else y_test.unique()

    # Tentukan tipe data evaluasi: Train atau Test
    data_type = "Train" if train else "Test"

    # Prediksi menggunakan model berdasarkan flag `train`
    pred_func = clf.predict(X_train) if train else clf.predict(X_test)

    # Buat classification report dan hitung akurasi
    clf_report = classification_report(y_train, pred_func) if train else classification_report(y_test, pred_func)
    accuracy = accuracy_score(y_train, pred_func) * 100 if train else accuracy_score(y_test, pred_func) * 100

    # Buat confusion matrix dari hasil prediksi
    cm = confusion_matrix(y_train, pred_func) if train else confusion_matrix(y_test, pred_func)

    # Visualisasi confusion matrix dengan heatmap
    plt.figure(figsize=(4, 2))
    sns.heatmap(
        cm,
        annot=True,
        cmap='Blues',
        fmt='g',
        xticklabels=[f'Predicted {class_name}' for class_name in classes],
        yticklabels=[f'Actual {class_name}' for class_name in classes]
    )

    # Tampilkan hasil evaluasi
    print(f"{data_type} Accuracy Score: {accuracy:.2f}%")
    print(f"CLASSIFICATION REPORT:\n{clf_report}")
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix ({data_type})')
    plt.show()

# Inisialisasi model Random Forest dengan hyperparameter spesifik
random_forest_model = RandomForestClassifier(
    n_estimators=50,      # Jumlah pohon dalam forest
    max_depth=5,          # Kedalaman maksimum tiap pohon
    max_features='sqrt',  # Jumlah fitur maksimum yang dipakai di tiap split
    random_state=42       # Seed untuk hasil yang reproducible
)

# Training model menggunakan data latih
random_forest_model.fit(X_train, y_train)

# Evaluasi performa model di data latih
evaluate(random_forest_model, X_train, y_train, X_test, y_test, train=True)

# Evaluasi performa model di data uji
evaluate(random_forest_model, X_train, y_train, X_test, y_test, train=False)

"""## Hyperparameter Tuning"""

# Inisialisasi model dasar Random Forest
base_model = RandomForestClassifier(random_state=123)

# Grid parameter untuk eksplorasi kombinasi hyperparameter terbaik
param_grid = {
    'n_estimators': [50, 100, 200, 300],             # Jumlah pohon dalam hutan
    'max_features': ['auto', 'sqrt', 'log2'],        # Jumlah fitur maksimum yang dipertimbangkan
    'max_depth': [5, 6, 7, 8],                        # Maksimum kedalaman pohon
    'criterion': ['gini', 'entropy']                 # Fungsi untuk mengukur kualitas split
}

# Grid Search dengan 5-fold cross-validation
grid_search = GridSearchCV(
    estimator=base_model,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,               # Gunakan semua core CPU yang tersedia
    verbose=1                # Tampilkan progres training
)

# Proses pencarian hyperparameter terbaik
grid_search.fit(X_train, y_train)

# Tampilkan kombinasi hyperparameter terbaik
print("Best Parameters:", grid_search.best_params_)

# Inisialisasi ulang model dengan hyperparameter terbaik dari GridSearchCV
best_rf_model = RandomForestClassifier(
    random_state=42,
    max_depth=grid_search.best_params_['max_depth'],
    n_estimators=grid_search.best_params_['n_estimators'],
    max_features=grid_search.best_params_['max_features'],
    criterion=grid_search.best_params_['criterion'],
    n_jobs=-1
)

# Fit model terbaik ke data latih
best_rf_model.fit(X_train, y_train)

# Evaluasi model pada data latih
evaluate(best_rf_model, X_train, y_train, X_test, y_test, train=True)

# Evaluasi model pada data uji
evaluate(best_rf_model, X_train, y_train, X_test, y_test, train=False)

"""# BEST MODEL

Berdasarkan hasil evaluasi model:

* Train Accuracy Score: 98.70%
* Test Accuracy Score: 75.32%
"""